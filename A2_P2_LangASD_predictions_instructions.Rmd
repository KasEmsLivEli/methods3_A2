---
title: "Assignment 2 - Language Development in ASD - Making predictions"
author: "Riccardo Fusaroli"
date: "August 9, 2019"
output: html_document
---


### Exercise 1) Testing model performance

```{r, include = FALSE}
pacman::p_load(readr,dplyr,stringr,lmerTest,Metrics,caret, lme4)

## Clean up function, included to inspire you

CleanUpData <- function(Demo,LU,Word){
  
  Speech <- merge(LU, Word) %>% 
    rename(
      Child.ID = SUBJ, 
      Visit=VISIT) %>%
    mutate(
      Visit = as.numeric(str_extract(Visit, "\\d")),
      Child.ID = gsub("\\.","", Child.ID)
      ) %>%
    dplyr::select(
      Child.ID, Visit, MOT_MLU, CHI_MLU, types_MOT, types_CHI, tokens_MOT, tokens_CHI
    )
  
  Demo <- Demo %>%
    dplyr::select(
      Child.ID, Visit, Ethnicity, Diagnosis, Gender, Age, ADOS, MullenRaw, ExpressiveLangRaw, Socialization
    ) %>%
    mutate(
      Child.ID = gsub("\\.","", Child.ID)
    )
    
  Data=merge(Demo,Speech,all=T)
  
  Data1= Data %>% 
     subset(Visit=="1") %>% 
     dplyr::select(Child.ID, ADOS, ExpressiveLangRaw, MullenRaw, Socialization) %>%
     rename(Ados1 = ADOS, 
            verbalIQ1 = ExpressiveLangRaw, 
            nonVerbalIQ1 = MullenRaw,
            Socialization1 = Socialization) 
  
  Data=merge(Data, Data1, all=T) %>%
    mutate(
      Child.ID = as.numeric(as.factor(as.character(Child.ID))),
      Visit = as.numeric(as.character(Visit)),
      Gender = recode(Gender, 
         "1" = "M",
         "2" = "F"),
      Diagnosis = recode(Diagnosis,
         "A"  = "ASD",
         "B"  = "TD")
    )

  return(Data)
}
```


```{r, include = FALSE}
# Load training Data
df <- read_csv("df_portfolio1.csv")

#remove NA's
train_sub_simple <- df %>%  dplyr::select("SUBJ","VISIT","Diagnosis","MOT_MLU","CHI_MLU")

train_sub_complex <- df %>%  dplyr::select("SUBJ","VISIT","Diagnosis","MOT_MLU","CHI_MLU","sev_aut_sympt","social_int_skills")

train_sub_simple <- train_sub_simple[complete.cases(train_sub_simple), ]
train_sub_complex <- train_sub_complex[complete.cases(train_sub_complex), ]

#- recreate the models you chose last time (just write the code again and apply it to Train Data)
chi_simple_train <- lmerTest::lmer(CHI_MLU ~ VISIT * Diagnosis + I(VISIT^2) + (1| SUBJ), data = train_sub_simple, REML = F)

chi_complex_train <- lmerTest::lmer(CHI_MLU ~ VISIT * sev_aut_sympt + social_int_skills + (1|SUBJ), data = train_sub_complex, REML = F)

#- calculate performance of the model on the training data: root mean square error is a good measure. (Tip: google the function rmse())
rmse(train_sub_simple$CHI_MLU, predict(chi_simple_train)) #0.394
rmse(train_sub_complex$CHI_MLU, predict(chi_complex_train)) #0.336

#- create the test dataset (apply the code from assignment 1 or my function to clean up the 3 test datasets)
# Test data
demo_test <- read_csv("demo_test.csv")
LU_test <- read_csv("LU_test.csv")
token_test <- read_csv("token_test.csv")

  #run Buyrakn's cleanup function
test <- CleanUpData(demo_test,LU_test,token_test)

test_sub_complex <- test %>%  dplyr::select("Child.ID", "Visit", "Diagnosis", "ADOS", "Socialization", "MOT_MLU", "CHI_MLU")
test_sub_simple <- test %>%  dplyr::select("Child.ID", "Visit", "Diagnosis", "MOT_MLU", "CHI_MLU")

  #renaming variables
test_sub_simple <- rename(test_sub_simple, VISIT = Visit)
test_sub_simple <- rename(test_sub_simple, SUBJ = Child.ID)

test_sub_complex <- rename(test_sub_complex, social_int_skills = Socialization)
test_sub_complex <- rename(test_sub_complex, sev_aut_sympt = ADOS)
test_sub_complex <- rename(test_sub_complex, VISIT = Visit)
test_sub_complex <- rename(test_sub_complex, SUBJ = Child.ID)

#removing NA's
test_sub_complex <- test_sub_complex[complete.cases(test_sub_complex), ]
test_sub_simple <- test_sub_simple[complete.cases(test_sub_simple), ]

#- test the performance of the models on the test data (Tips: google the functions "predict()")

chi_simple_test <- lmerTest::lmer(CHI_MLU ~ VISIT * Diagnosis + I(VISIT^2) + (1| SUBJ), data = test_sub_simple, REML = F)

chi_complex_test <- lmerTest::lmer(CHI_MLU ~ VISIT * sev_aut_sympt + social_int_skills + (1|SUBJ), data = test_sub_complex, REML = F)


rmse(test_sub_simple$CHI_MLU, predict(chi_simple_test)) #0.330
rmse(test_sub_complex$CHI_MLU, predict(chi_complex_test)) #0.233

#- optional: predictions are never certain, can you identify the uncertainty of the predictions? (e.g. google predictinterval())
pacman::p_load(merTools)

predictInterval(chi_complex_train)
predictInterval(chi_simple_train)

predictInterval(chi_complex_test)
predictInterval(chi_simple_test)

#predictInterval df = the lower and upper limits of the prediction interval and the mean or median of the simulated predictions
```

### Exercise 2) Model Selection via Cross-validation (N.B: ChildMLU!)

```{r}
#- Create the basic model of ChildMLU as a function of Time and Diagnosis (don't forget the random effects!). 
BM_train <- lmerTest::lmer(CHI_MLU ~ VISIT * Diagnosis + (1 + VISIT | SUBJ), data = train_sub_simple, REML = F)

BM_test <- lmerTest::lmer(CHI_MLU ~ VISIT * Diagnosis + (1 + VISIT | SUBJ), data = test_sub_simple, REML = F)

#- Make a cross-validated version of the model. (Tips: google the function "createFolds";  loop through each fold, train a model on the other folds and test it on the fold)

#because we wanna cross-validate on as much data as possible, we first combine the training and test datasets
big_df <- rbind(train_sub_simple, test_sub_simple)
```


```{r}
#doing a for-loop to cross-validate on the big df using the basic model
k = 5
folds = createFolds(unique(big_df$SUBJ), k = k, list = TRUE, returnTrain = FALSE)

trainRMSE = rep(NA, k)
testRMSE = rep(NA, k)

i = 1

for (fold in folds){
  train = subset(big_df, !(SUBJ %in% fold)) #creating a subset without ID values
  test = subset(big_df, SUBJ %in% fold)
  model = lmerTest::lmer(CHI_MLU ~ VISIT * Diagnosis + (1 + VISIT | SUBJ), data = train, REML = FALSE)
  
  test$prediction = predict(model, test, allow.new.levels = TRUE)
  train$prediction = fitted(model)
  
  trainRMSE[i] = rmse(train$CHI_MLU, fitted(model))
  testRMSE[i] = rmse(test$CHI_MLU, test$prediction)
  i = i + 1
}

#doing a for-loop to cross-validate on the big df using the simple model
trainRMSE_simple = rep(NA, k)
testRMSE_simple = rep(NA, k)

for (fold in folds){
  train_simple = subset(big_df, !(SUBJ %in% fold)) #creating a subset without ID values
  test_simple = subset(big_df, SUBJ %in% fold)
  model_simple = lmerTest::lmer(CHI_MLU ~ VISIT * Diagnosis + I(VISIT^2) + (1| SUBJ), data = train_simple, REML = FALSE)
  
  test_simple$prediction = predict(model_simple, test_simple, allow.new.levels = TRUE)
  train_simple$prediction = fitted(model_simple)
  
  trainRMSE_simple[i] = rmse(train_simple$CHI_MLU, fitted(model_simple))
  testRMSE_simple[i] = rmse(test_simple$CHI_MLU, test_simple$prediction)
  i = i + 1
}

#doing a for-loop to cross-validate on the big df using the complex model
  #create big_df_complex
big_df_complex <- rbind(train_sub_complex, test_sub_complex)

k = 7
folds_complex = createFolds(unique(big_df_complex$SUBJ), k = k, list = TRUE, returnTrain = FALSE)
trainRMSE_complex = rep(NA, k)
testRMSE_complex = rep(NA, k)

for (fold in folds_complex){
  train_complex = subset(big_df_complex, !(SUBJ %in% fold)) #creating a subset without ID values
  test_complex = subset(big_df_complex, SUBJ %in% fold)
  model_complex = lmerTest::lmer(CHI_MLU ~ VISIT * sev_aut_sympt + social_int_skills + (1|SUBJ), data = train_complex, REML = FALSE)
  
  test_complex$prediction = predict(model_complex, test_complex, allow.new.levels = TRUE)
  train_complex$prediction = fitted(model_complex)
  
  trainRMSE_complex[i] = rmse(train_complex$CHI_MLU, fitted(model_complex))
  testRMSE_complex[i] = rmse(test_complex$CHI_MLU, test_complex$prediction)
  i = i + 1
}

#- Report the results and comment on them.
trainRMSE
testRMSE

trainRMSE_simple
testRMSE_simple

trainRMSE_complex
testRMSE_complex

```


### Exercise 3) Assessing the single child


```{r}
#first we found Bernie, he had ID=2, however, he has been extracted when we removed NA's
b <- CleanUpData(demo_test,LU_test,token_test)
bernie <- subset(b, Child.ID == 2)

bernie <- bernie %>%  dplyr::select("Child.ID", "Visit", "Diagnosis", "MOT_MLU", "CHI_MLU", "ADOS", "Socialization")

  #renaming variables
bernie <- rename(bernie, VISIT = Visit)
bernie <- rename(bernie, SUBJ = Child.ID)
bernie <- rename(bernie, social_int_skills = Socialization)
bernie <- rename(bernie, sev_aut_sympt = ADOS)

#bernie's mlu compared to an averge TD kid mlu
bernie$SUBJ <- as.character(bernie$SUBJ)
td <- subset(big_df, Diagnosis == "TD")
bernie <- full_join(td, bernie)

ggplot(bernie, aes(x = VISIT, y = CHI_MLU, fill = Diagnosis, shape = Diagnosis, col = Diagnosis)) +
  geom_point() +
  geom_smooth(method="lm") + labs(x="Visit", y="Child MLU", title = "Bernie compared to TD children")

#use predict() function on subset and see if prediction matches actual value

bernie_model <- lmerTest::lmer(CHI_MLU ~ VISIT * Diagnosis + I(VISIT^2) + (1| SUBJ), data = train_simple, REML = FALSE)
p2 <- stats::predict(bernie_model, bernie)
p2

```

